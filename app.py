import streamlit as st
import os
import warnings
from datetime import datetime
warnings.filterwarnings("ignore")

st.set_page_config(
    page_title="RAG Soru-Cevap",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .user-message {
        background-color: #e3f2fd;
        padding: 15px;
        border-radius: 10px;
        margin: 10px 0;
        border-left: 4px solid #2196F3;
    }
    .assistant-message {
        background-color: #f5f5f5;
        padding: 15px;
        border-radius: 10px;
        margin: 10px 0;
        border-left: 4px solid #4CAF50;
    }
    .source-box {
        background-color: #fff9e6;
        padding: 10px;
        border-radius: 5px;
        margin: 5px 0;
        font-size: 0.9em;
    }
    .stButton>button {
        width: 100%;
    }
</style>
""", unsafe_allow_html=True)

st.title("üìö RAG Soru-Cevap Sistemi")
st.markdown("*PDF dok√ºmanlarƒ±nƒ±zƒ± y√ºkleyin ve sorular sorun*")
st.markdown("---")

# Session state initialization
if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'qa_chain' not in st.session_state:
    st.session_state.qa_chain = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'pdf_name' not in st.session_state:
    st.session_state.pdf_name = None

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è Ayarlar")
    
    groq_api_key = st.text_input(
        "üîë Groq API Key",
        type="password",
        placeholder="gsk_...",
        help="Groq API anahtarƒ±nƒ±zƒ± girin"
    )
    
    if groq_api_key:
        os.environ["GROQ_API_KEY"] = groq_api_key
        st.success("‚úÖ API Key ayarlandƒ±")
    
    st.markdown("---")
    
    # PDF Upload Section
    if groq_api_key:
        st.subheader("üìÑ PDF Y√ºkle")
        
        st.caption("Maksimum: 500MB | √ñnerilen: 50MB altƒ±")
        
        uploaded_file = st.file_uploader("PDF se√ßin", type=['pdf'])
        
        if uploaded_file:
            file_size_mb = uploaded_file.size / (1024 * 1024)
            st.info(f"üìä Dosya: {uploaded_file.name}")
            st.info(f"üìè Boyut: {file_size_mb:.2f} MB")
            
            if file_size_mb > 50:
                st.warning("‚ö†Ô∏è B√ºy√ºk dosya! ƒ∞≈ülem uzun s√ºrebilir.")
            
            process_btn = st.button("üöÄ PDF'i ƒ∞≈üle", type="primary", use_container_width=True)
            
            if process_btn:
                progress = st.progress(0)
                status = st.empty()
                
                try:
                    from langchain_community.document_loaders import PyPDFLoader
                    from langchain.text_splitter import RecursiveCharacterTextSplitter
                    from langchain_community.vectorstores import FAISS
                    from langchain_groq import ChatGroq
                    from langchain.chains import RetrievalQA
                    from langchain.prompts import PromptTemplate
                    
                    # Step 1: Save PDF
                    status.text("1/5 üì• PDF kaydediliyor...")
                    progress.progress(20)
                    with open("temp.pdf", "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    # Step 2: Load PDF
                    status.text("2/5 üìñ PDF okunuyor...")
                    progress.progress(40)
                    loader = PyPDFLoader("temp.pdf")
                    documents = loader.load()
                    st.info(f"‚úÖ {len(documents)} sayfa okundu")
                    
                    # Step 3: Split documents
                    status.text("3/5 ‚úÇÔ∏è Par√ßalara ayrƒ±lƒ±yor...")
                    progress.progress(60)
                    
                    # Dynamic chunk size based on file size
                    if file_size_mb > 20:
                        chunk_size = 1200
                        chunk_overlap = 150
                    elif file_size_mb > 10:
                        chunk_size = 1000
                        chunk_overlap = 100
                    else:
                        chunk_size = 800
                        chunk_overlap = 100
                    
                    splitter = RecursiveCharacterTextSplitter(
                        chunk_size=chunk_size,
                        chunk_overlap=chunk_overlap,
                        separators=["\n\n", "\n", " ", ""]
                    )
                    chunks = splitter.split_documents(documents)
                    st.info(f"‚úÖ {len(chunks)} par√ßa olu≈üturuldu")
                    
                    # Step 4: Create embeddings
                    status.text("4/5 üß† Embedding modeli y√ºkleniyor...")
                    progress.progress(75)
                    
                    try:
                        from sentence_transformers import SentenceTransformer
                        from langchain.embeddings.base import Embeddings
                        
                        class SentenceTransformerEmbeddings(Embeddings):
                            _model = None
                            
                            def __init__(self, model_name="all-MiniLM-L6-v2"):
                                if SentenceTransformerEmbeddings._model is None:
                                    SentenceTransformerEmbeddings._model = SentenceTransformer(
                                        model_name,
                                        device='cpu'
                                    )
                                self.model = SentenceTransformerEmbeddings._model
                            
                            def embed_documents(self, texts):
                                embeddings = self.model.encode(
                                    texts, 
                                    show_progress_bar=False,
                                    convert_to_numpy=True
                                )
                                return embeddings.tolist()
                            
                            def embed_query(self, text):
                                embedding = self.model.encode(
                                    [text], 
                                    show_progress_bar=False,
                                    convert_to_numpy=True
                                )
                                return embedding[0].tolist()
                        
                        embeddings = SentenceTransformerEmbeddings()
                        st.success("‚úÖ Sentence Transformers y√ºklendi (Semantic Search)")
                        
                    except Exception as e:
                        st.warning("‚ö†Ô∏è Sentence-transformers y√ºklenemedi, TF-IDF kullanƒ±lƒ±yor")
                        
                        from sklearn.feature_extraction.text import TfidfVectorizer
                        from langchain.embeddings.base import Embeddings
                        
                        class SimpleEmbeddings(Embeddings):
                            def __init__(self):
                                self.vectorizer = TfidfVectorizer(max_features=384)
                                self.fitted = False
                            
                            def embed_documents(self, texts):
                                if not self.fitted:
                                    self.vectorizer.fit(texts)
                                    self.fitted = True
                                vectors = self.vectorizer.transform(texts).toarray()
                                return vectors.tolist()
                            
                            def embed_query(self, text):
                                if not self.fitted:
                                    return [0.0] * 384
                                vector = self.vectorizer.transform([text]).toarray()
                                return vector[0].tolist()
                        
                        embeddings = SimpleEmbeddings()
                        st.info("‚úÖ TF-IDF kullanƒ±lƒ±yor")
                    
                    # Create vector store
                    vectorstore = FAISS.from_documents(chunks, embeddings)
                    st.session_state.vectorstore = vectorstore
                    
                    # Step 5: Setup LLM and QA Chain
                    status.text("5/5 ü§ñ LLM hazƒ±rlanƒ±yor...")
                    progress.progress(90)
                    
                    llm = ChatGroq(
                        model="llama-3.3-70b-versatile",
                        temperature=0.2
                    )
                    
                    prompt = PromptTemplate(
                        template="""Sen bir uzman asistansƒ±n. Sadece verilen baƒülam i√ßindeki bilgileri kullanarak cevap ver.

BAƒûLAM:
{context}

SORU: {question}

KURALLAR:
- Sadece baƒülamdaki bilgileri kullan
- Bilgi yoksa "Bu bilgi dok√ºmanda bulunmuyor" de
- T√ºrk√ße, net ve anla≈üƒ±lƒ±r cevap ver
- M√ºmk√ºnse sayfa numarasƒ± belirt

CEVAP:""",
                        input_variables=["context", "question"]
                    )
                    
                    qa_chain = RetrievalQA.from_chain_type(
                        llm=llm,
                        retriever=vectorstore.as_retriever(
                            search_kwargs={"k": 3}
                        ),
                        return_source_documents=True,
                        chain_type_kwargs={"prompt": prompt}
                    )
                    st.session_state.qa_chain = qa_chain
                    st.session_state.pdf_name = uploaded_file.name
                    
                    # Cleanup
                    if os.path.exists("temp.pdf"):
                        os.remove("temp.pdf")
                    
                    progress.progress(100)
                    status.empty()
                    progress.empty()
                    
                    st.success(f"‚úÖ Sistem hazƒ±r! {len(chunks)} par√ßa olu≈üturuldu")
                    st.balloons()
                    
                    # Clear chat history on new PDF
                    st.session_state.chat_history = []
                    
                except Exception as e:
                    st.error(f"‚ùå Hata: {str(e)}")
                    import traceback
                    with st.expander("Hata detaylarƒ±"):
                        st.code(traceback.format_exc())
    
    # System status
    st.markdown("---")
    if st.session_state.qa_chain:
        st.success("‚úÖ Sistem Aktif")
        if st.session_state.pdf_name:
            st.info(f"üìÑ {st.session_state.pdf_name}")
        
        # Clear chat button
        if st.button("üóëÔ∏è Sohbeti Temizle", use_container_width=True):
            st.session_state.chat_history = []
            st.rerun()
        
        # Stats
        if st.session_state.chat_history:
            st.markdown("---")
            st.metric("Soru Sayƒ±sƒ±", len(st.session_state.chat_history))

# Main area
if not groq_api_key:
    st.info("üëà L√ºtfen Groq API Key girin")
    st.markdown("""
    ### Nasƒ±l API Key alƒ±nƒ±r?
    1. [console.groq.com](https://console.groq.com) adresine gidin
    2. √úcretsiz hesap olu≈üturun
    3. API Keys b√∂l√ºm√ºnden yeni key olu≈üturun
    """)
    
elif not st.session_state.qa_chain:
    st.info("üëà L√ºtfen PDF y√ºkleyip i≈üleyin")
    st.markdown("""
    ### PDF formatƒ±nda belge y√ºkleyin
    - Maksimum dosya boyutu: 500MB
    - √ñnerilen: 50MB altƒ± dosyalar daha hƒ±zlƒ± i≈ülenir
    """)
    
else:
    # Chat interface
    st.subheader("üí¨ Sohbet")
    
    # Display chat history
    chat_container = st.container()
    with chat_container:
        if st.session_state.chat_history:
            for i, chat in enumerate(st.session_state.chat_history):
                # User message
                st.markdown(
                    f"""<div class="user-message">
                    <strong>üë§ Sen:</strong><br>{chat['question']}
                    </div>""",
                    unsafe_allow_html=True
                )
                
                # Assistant message
                st.markdown(
                    f"""<div class="assistant-message">
                    <strong>ü§ñ Asistan:</strong><br>{chat['answer']}
                    </div>""",
                    unsafe_allow_html=True
                )
                
                # Sources in expander
                if chat.get('sources'):
                    with st.expander(f"üìö Kaynaklar ({len(chat['sources'])} adet)"):
                        for j, source in enumerate(chat['sources'], 1):
                            page_num = source.metadata.get('page', 'N/A')
                            st.markdown(f"""
                            <div class="source-box">
                            <strong>Kaynak {j} (Sayfa: {page_num})</strong><br>
                            {source.page_content[:300]}...
                            </div>
                            """, unsafe_allow_html=True)
                
                st.markdown("---")
        else:
            st.info("üëã Merhaba! PDF'iniz hakkƒ±nda soru sorabilirsiniz.")
    
    # Question input area
    st.markdown("### üí≠ Yeni Soru")
    
    col1, col2 = st.columns([4, 1])
    
    with col1:
        question = st.text_input(
            "Sorunuz:",
            placeholder="Bu d√∂k√ºman ne hakkƒ±nda?",
            key="question_input",
            label_visibility="collapsed"
        )
    
    with col2:
        ask_button = st.button("üîç Sor", type="primary", use_container_width=True)
    
    # Example questions
    st.markdown("**üí° √ñrnek Sorular:**")
    example_cols = st.columns(3)
    
    examples = [
        "Bu d√∂k√ºman ne hakkƒ±nda?",
        "Ana konularƒ± √∂zetle",
        "En √∂nemli noktalar neler?"
    ]
    
    example_clicked = None
    for idx, ex in enumerate(examples):
        with example_cols[idx]:
            if st.button(ex, key=f"example_{idx}", use_container_width=True):
                example_clicked = ex
    
    # Handle question (from input or example)
    query = None
    if ask_button and question:
        query = question
    elif example_clicked:
        query = example_clicked
    
    if query:
        with st.spinner("ü§î D√º≈ü√ºn√ºyorum..."):
            try:
                result = st.session_state.qa_chain.invoke({"query": query})
                
                # Add to chat history
                st.session_state.chat_history.append({
                    'question': query,
                    'answer': result['result'],
                    'sources': result.get('source_documents', []),
                    'timestamp': datetime.now().strftime("%H:%M:%S")
                })
                
                # Rerun to show new message
                st.rerun()
                
            except Exception as e:
                st.error(f"‚ùå Hata olu≈ütu: {str(e)}")

# Footer
st.markdown("---")
st.caption("üìö RAG Soru-Cevap Sistemi | Powered by Groq + LangChain")